{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ejpark78/codelab/blob/master/bert/huggingface_konlpy/huggingface_konlpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"172.19.153.41 nlp-utils\" >> /etc/hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://k8s:****@nlp-utils/repository/pypi/simple\n",
      "Collecting GitPython==3.1.9\n",
      "  Downloading GitPython-3.1.9-py3-none-any.whl (159 kB)\n",
      "\u001b[K     |████████████████████████████████| 159 kB 296 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
      "  Downloading https://nlp-utils/repository/pypi/packages/gitdb/4.0.5/gitdb-4.0.5-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 5.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
      "  Downloading https://nlp-utils/repository/pypi/packages/smmap/3.0.4/smmap-3.0.4-py2.py3-none-any.whl (25 kB)\n",
      "Installing collected packages: smmap, gitdb, GitPython\n",
      "Successfully installed GitPython-3.1.9 gitdb-4.0.5 smmap-3.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install https://files.pythonhosted.org/packages/c0/d7/b2b0672e0331567157adf9281f41ee731c412ee518ca5e6552c27fa73c91/GitPython-3.1.9-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://k8s:****@nlp-utils/repository/pypi/simple\n",
      "Requirement already satisfied: konlpy>=0.5.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.5.2)\n",
      "Collecting tokenizers==0.8.1\n",
      "  Using cached https://nlp-utils/repository/pypi/packages/tokenizers/0.8.1/tokenizers-0.8.1-cp36-cp36m-manylinux1_x86_64.whl (3.0 MB)\n",
      "Collecting transformers==3.0.2\n",
      "  Using cached https://nlp-utils/repository/pypi/packages/transformers/3.0.2/transformers-3.0.2-py3-none-any.whl (769 kB)\n",
      "Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (4.50.2)\n",
      "Collecting wandb\n",
      "  Using cached https://nlp-utils/repository/pypi/packages/wandb/0.10.7/wandb-0.10.7-py2.py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy>=0.5.2->-r requirements.txt (line 1)) (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy>=0.5.2->-r requirements.txt (line 1)) (1.18.5)\n",
      "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy>=0.5.2->-r requirements.txt (line 1)) (3.9.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy>=0.5.2->-r requirements.txt (line 1)) (1.0.2)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy>=0.5.2->-r requirements.txt (line 1)) (4.5.2)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy>=0.5.2->-r requirements.txt (line 1)) (0.4.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->-r requirements.txt (line 3)) (2.24.0)\n",
      "Collecting sentencepiece!=0.1.92\n",
      "  Using cached https://nlp-utils/repository/pypi/packages/sentencepiece/0.1.91/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->-r requirements.txt (line 3)) (0.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->-r requirements.txt (line 3)) (20.4)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->-r requirements.txt (line 3)) (0.0.43)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->-r requirements.txt (line 3)) (2020.10.11)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->-r requirements.txt (line 3)) (3.0.12)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb->-r requirements.txt (line 5)) (3.1.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb->-r requirements.txt (line 5)) (2.8.1)\n",
      "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.6/dist-packages (from wandb->-r requirements.txt (line 5)) (5.0.1)\n",
      "Collecting sentry-sdk>=0.4.0\n",
      "  Using cached https://nlp-utils/repository/pypi/packages/sentry-sdk/0.19.0/sentry_sdk-0.19.0-py2.py3-none-any.whl (120 kB)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.6/dist-packages (from wandb->-r requirements.txt (line 5)) (3.12.2)\n",
      "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb->-r requirements.txt (line 5)) (7.1.2)\n",
      "Collecting promise<3,>=2.0\n",
      "  Using cached https://nlp-utils/repository/pypi/packages/promise/2.3/promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from wandb->-r requirements.txt (line 5)) (1.15.0)\n",
      "Collecting watchdog>=0.8.3\n",
      "  Using cached https://nlp-utils/repository/pypi/packages/watchdog/0.10.3/watchdog-0.10.3.tar.gz (94 kB)\n",
      "Collecting psutil>=5.0.0\n",
      "  Using cached https://nlp-utils/repository/pypi/packages/psutil/5.7.2/psutil-5.7.2.tar.gz (460 kB)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from wandb->-r requirements.txt (line 5)) (5.3.1)\n",
      "Collecting subprocess32>=3.5.3\n",
      "  Using cached https://nlp-utils/repository/pypi/packages/subprocess32/3.5.4/subprocess32-3.5.4.tar.gz (97 kB)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Using cached https://nlp-utils/repository/pypi/packages/docker-pycreds/0.4.0/docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Using cached https://nlp-utils/repository/pypi/packages/shortuuid/1.0.1/shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy>=0.5.2->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy>=0.5.2->-r requirements.txt (line 1)) (3.7.4.3)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2->-r requirements.txt (line 3)) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2->-r requirements.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==3.0.2->-r requirements.txt (line 3)) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2->-r requirements.txt (line 3)) (2020.6.20)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2->-r requirements.txt (line 3)) (2.4.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2->-r requirements.txt (line 3)) (0.14.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython>=1.0.0->wandb->-r requirements.txt (line 5)) (4.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.12.0->wandb->-r requirements.txt (line 5)) (50.3.0)\n",
      "Collecting pathtools>=0.1.1\n",
      "  Downloading https://nlp-utils/repository/pypi/packages/pathtools/0.1.2/pathtools-0.1.2.tar.gz (11 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy>=0.5.2->-r requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb->-r requirements.txt (line 5)) (3.0.4)\n",
      "Building wheels for collected packages: promise, watchdog, psutil, subprocess32, pathtools\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=dee3cc28ccf45347f7843679d6d4acf5698e04e616361fb24579e8df9bdeecf0\n",
      "  Stored in directory: /root/.cache/pip/wheels/a1/58/48/c514bd0ae0e6e4a712bce359be8409f323a9508a98cd323d06\n",
      "  Building wheel for watchdog (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for watchdog: filename=watchdog-0.10.3-py3-none-any.whl size=73868 sha256=a54fe4a38a55fc38f4518d66e79c06667bb0fa91a74ee01c8af1efbe1ed33196\n",
      "  Stored in directory: /root/.cache/pip/wheels/c9/a6/71/b3d6484c3c651cff24264381d6e3ffc04eecc017356bdb3e47\n",
      "  Building wheel for psutil (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for psutil: filename=psutil-5.7.2-cp36-cp36m-linux_x86_64.whl size=279881 sha256=306586b4e6553c266ec20d11e815237a57b314219e2a613f34542358a076c084\n",
      "  Stored in directory: /root/.cache/pip/wheels/48/a2/24/eef4a3ff5c55b0bf4679e9f3fb427d5d713af2aaa45803c7c0\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6487 sha256=934f3f22254a571a2291887054df047adcdaaf93c3b874878def3f08432dc7ca\n",
      "  Stored in directory: /root/.cache/pip/wheels/47/0a/23/fbbaba0666e6b2cb40b48d8de7606588055530a1534146183b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8785 sha256=8bfe63ae1783ff3e809fd60c11a38f3a5031dfa808d4e1dcb54a1400f1e21d65\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/d2/8f/d3f5790ef36c90525743889dd3d7ba0dcbec9ffe48a0fe6f10\n",
      "Successfully built promise watchdog psutil subprocess32 pathtools\n",
      "Installing collected packages: tokenizers, sentencepiece, transformers, sentry-sdk, promise, pathtools, watchdog, psutil, subprocess32, docker-pycreds, shortuuid, wandb\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.7.0\n",
      "    Uninstalling tokenizers-0.7.0:\n",
      "      Successfully uninstalled tokenizers-0.7.0\n",
      "  Attempting uninstall: sentencepiece\n",
      "    Found existing installation: sentencepiece 0.1.92\n",
      "    Uninstalling sentencepiece-0.1.92:\n",
      "      Successfully uninstalled sentencepiece-0.1.92\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 2.11.0\n",
      "    Uninstalling transformers-2.11.0:\n",
      "      Successfully uninstalled transformers-2.11.0\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "transformers 3.0.2 requires tokenizers==0.8.1.rc1, but you'll have tokenizers 0.8.1 which is incompatible.\u001b[0m\n",
      "Successfully installed docker-pycreds-0.4.0 pathtools-0.1.2 promise-2.3 psutil-5.7.2 sentencepiece-0.1.91 sentry-sdk-0.19.0 shortuuid-1.0.1 subprocess32-3.5.4 tokenizers-0.8.1 transformers-3.0.2 wandb-0.10.7 watchdog-0.10.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xUF5cWOjU7o"
   },
   "source": [
    "# 튜토리얼 코스 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JfpBqvYSdzW7",
    "outputId": "03683c09-ad09-4e8d-8a6c-e7e898533b99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-16 20:56:59.457171: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
      "10/16/2020 20:57:01 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
      "10/16/2020 20:57:01 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "10/16/2020 20:57:01 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='output', overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=0.0001, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Oct16_20-57-01_21dd1781cf2b', logging_first_step=False, logging_steps=500, save_steps=2000, save_total_limit=2, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)\n",
      "10/16/2020 20:57:01 - INFO - transformers.configuration_utils -   loading configuration file examples/bert_config.json\n",
      "10/16/2020 20:57:01 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "10/16/2020 20:57:01 - INFO - __main__ -   Training new model from scratch\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/modeling_auto.py:709: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "10/16/2020 20:57:03 - INFO - filelock -   Lock 139950301617568 acquired on data/cached_lm_KoNLPyBertTokenizer_252_2020-07-29_covid_news_sents.txt.lock\n",
      "10/16/2020 20:57:03 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at data\n",
      "10/16/2020 20:57:18 - INFO - transformers.data.datasets.language_modeling -   Saving features into cached file data/cached_lm_KoNLPyBertTokenizer_252_2020-07-29_covid_news_sents.txt [took 0.053 s]\n",
      "10/16/2020 20:57:18 - INFO - filelock -   Lock 139950301617568 released on data/cached_lm_KoNLPyBertTokenizer_252_2020-07-29_covid_news_sents.txt.lock\n",
      "10/16/2020 20:57:20 - INFO - transformers.trainer -   You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
      "MODELPATH = None\n",
      "10/16/2020 20:57:20 - INFO - transformers.trainer -   ***** Running training *****\n",
      "10/16/2020 20:57:20 - INFO - transformers.trainer -     Num examples = 12149\n",
      "10/16/2020 20:57:20 - INFO - transformers.trainer -     Num Epochs = 3\n",
      "10/16/2020 20:57:20 - INFO - transformers.trainer -     Instantaneous batch size per device = 8\n",
      "10/16/2020 20:57:20 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "10/16/2020 20:57:20 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
      "10/16/2020 20:57:20 - INFO - transformers.trainer -     Total optimization steps = 4557\n",
      "Epoch:   0%|                                              | 0/3 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                       | 0/1519 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   0%|                               | 1/1519 [00:00<05:40,  4.45it/s]\u001b[A\n",
      "Iteration:   0%|                               | 2/1519 [00:00<05:30,  4.59it/s]\u001b[A\n",
      "Iteration:   0%|                               | 3/1519 [00:00<05:22,  4.71it/s]\u001b[A\n",
      "Iteration:   0%|                               | 4/1519 [00:00<05:21,  4.71it/s]\u001b[A\n",
      "Iteration:   0%|                               | 5/1519 [00:01<05:13,  4.82it/s]\u001b[A\n",
      "Iteration:   0%|                               | 6/1519 [00:01<05:10,  4.88it/s]\u001b[A\n",
      "Iteration:   0%|▏                              | 7/1519 [00:01<05:07,  4.92it/s]\u001b[A\n",
      "Iteration:   1%|▏                              | 8/1519 [00:01<05:06,  4.92it/s]\u001b[A\n",
      "Iteration:   1%|▏                              | 9/1519 [00:01<05:06,  4.93it/s]\u001b[A\n",
      "Iteration:   1%|▏                             | 10/1519 [00:02<05:03,  4.97it/s]\u001b[A\n",
      "Iteration:   1%|▏                             | 11/1519 [00:02<05:03,  4.97it/s]\u001b[A\n",
      "Iteration:   1%|▏                             | 12/1519 [00:02<05:01,  4.99it/s]\u001b[A\n",
      "Iteration:   1%|▎                             | 13/1519 [00:02<05:00,  5.02it/s]\u001b[A\n",
      "Iteration:   1%|▎                             | 14/1519 [00:02<04:58,  5.03it/s]\u001b[A\n",
      "Iteration:   1%|▎                             | 15/1519 [00:03<04:58,  5.04it/s]\u001b[A\n",
      "Iteration:   1%|▎                             | 16/1519 [00:03<04:57,  5.05it/s]\u001b[A\n",
      "Iteration:   1%|▎                             | 17/1519 [00:03<04:57,  5.06it/s]\u001b[A\n",
      "Iteration:   1%|▎                             | 18/1519 [00:03<04:58,  5.02it/s]\u001b[A\n",
      "Iteration:   1%|▍                             | 19/1519 [00:03<04:58,  5.03it/s]\u001b[A\n",
      "Iteration:   1%|▍                             | 20/1519 [00:04<04:56,  5.05it/s]\u001b[A\n",
      "Iteration:   1%|▍                             | 21/1519 [00:04<04:57,  5.04it/s]\u001b[A\n",
      "Iteration:   1%|▍                             | 22/1519 [00:04<04:57,  5.04it/s]\u001b[A\n",
      "Iteration:   2%|▍                             | 23/1519 [00:04<04:55,  5.07it/s]\u001b[A\n",
      "Iteration:   2%|▍                             | 24/1519 [00:04<04:54,  5.07it/s]\u001b[A\n",
      "Iteration:   2%|▍                             | 25/1519 [00:05<04:53,  5.09it/s]\u001b[A\n",
      "Iteration:   2%|▌                             | 26/1519 [00:05<04:57,  5.01it/s]\u001b[A\n",
      "Iteration:   2%|▌                             | 27/1519 [00:05<04:56,  5.03it/s]\u001b[A\n",
      "Iteration:   2%|▌                             | 28/1519 [00:05<04:57,  5.02it/s]\u001b[A\n",
      "Iteration:   2%|▌                             | 29/1519 [00:05<04:59,  4.97it/s]\u001b[A\n",
      "Iteration:   2%|▌                             | 30/1519 [00:06<05:00,  4.95it/s]\u001b[A\n",
      "Iteration:   2%|▌                             | 31/1519 [00:06<04:58,  4.98it/s]\u001b[A\n",
      "Iteration:   2%|▋                             | 32/1519 [00:06<05:04,  4.88it/s]\u001b[A\n",
      "Iteration:   2%|▋                             | 33/1519 [00:06<04:59,  4.95it/s]\u001b[A\n",
      "Iteration:   2%|▋                             | 34/1519 [00:06<04:57,  4.99it/s]\u001b[A\n",
      "Iteration:   2%|▋                             | 35/1519 [00:07<04:57,  4.99it/s]\u001b[A\n",
      "Iteration:   2%|▋                             | 36/1519 [00:07<04:54,  5.03it/s]\u001b[A^C\n",
      "Iteration:   2%|▋                             | 36/1519 [00:07<05:04,  4.87it/s]\n",
      "Epoch:   0%|                                              | 0/3 [00:07<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"examples/run_konlpy_bert.py\", line 307, in <module>\n",
      "    main()\n",
      "  File \"examples/run_konlpy_bert.py\", line 271, in main\n",
      "    trainer.train(model_path=model_path)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/transformers/trainer.py\", line 499, in train\n",
      "    tr_loss += self._training_step(model, inputs, optimizer)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/transformers/trainer.py\", line 637, in _training_step\n",
      "    loss.backward()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/tensor.py\", line 185, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\", line 127, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "KeyboardInterrupt\n",
      "10/16/2020 20:57:27 - INFO - wandb.internal.internal -   Internal process exited\n"
     ]
    }
   ],
   "source": [
    "!PYTHONPATH=. python examples/run_konlpy_bert.py \\\n",
    "    --output_dir='output' \\\n",
    "    --overwrite_output_dir \\\n",
    "    --model_type=bert \\\n",
    "    --config_name='examples/bert_config.json' \\\n",
    "    --mlm \\\n",
    "    --vocab_file='tutorials/tokenizers/BertStyleMecab/notag-vocab.txt' \\\n",
    "    --do_train \\\n",
    "    --train_data_file='data/2020-07-29_covid_news_sents.txt' \\\n",
    "    --konlpy_name=mecab \\\n",
    "    --learning_rate=1e-4 \\\n",
    "    --num_train_epochs=3 \\\n",
    "    --save_total_limit=2 \\\n",
    "    --save_steps=2000 \\\n",
    "    --block_size=254 \\\n",
    "    --seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Allocations           |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO4M3BPtfpH0IEkNI6aJquF",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "huggingface_konlpy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
