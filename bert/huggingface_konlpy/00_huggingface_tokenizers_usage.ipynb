{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ejpark78/codelab/blob/master/bert/huggingface_konlpy/00_huggingface_tokenizers_usage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"172.19.153.41 nlp-utils\" >> /etc/hosts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yptqj7Abv3H9"
   },
   "source": [
    "출처: https://github.com/lovit/huggingface_konlpy/blob/master/tutorials/00_huggingface_tokenizers_usage.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IbTTfXCsvBWB",
    "outputId": "654a6026-68c1-4171-edb2-aecad903e386"
   },
   "outputs": [],
   "source": [
    "# git clone\n",
    "!git clone https://github.com/lovit/huggingface_konlpy.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r huggingface_konlpy/requirements.txt\n",
    "!pip freeze | grep transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==3.0.2 \n",
    "!pip install tokenizers==0.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "92R0sf6lu2eO",
    "outputId": "37ffa4f2-5c35-41a1-9898-277ae8fa5e0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.8.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tokenizers\n",
    "tokenizers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C43_IkS-u2eb"
   },
   "outputs": [],
   "source": [
    "from tokenizers import (ByteLevelBPETokenizer,\n",
    "                        CharBPETokenizer,\n",
    "                        SentencePieceBPETokenizer,\n",
    "                        BertWordPieceTokenizer)\n",
    "\n",
    "small_corpus = './data/very_small_corpus.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlOX6Y4nu2ej"
   },
   "source": [
    "## Bert WordPiece Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "44hkKpHqu2eo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', '##B', '##C', '##D', '##E']\n",
      "[5, 11, 13, 12, 14]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer(lowercase = False)\n",
    "bert_wordpiece_tokenizer.train(files=[small_corpus], vocab_size=10)\n",
    "\n",
    "encoding = bert_wordpiece_tokenizer.encode('ABCDE')\n",
    "\n",
    "print(encoding.tokens)\n",
    "print(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "__tG4vIxu2e1",
    "outputId": "5625a61f-f0f6-4644-d207-c293abf82775"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', 'a', 'b', 'c', 'd', 'e', 'f', '##c', '##b', '##d', '##e', '##f']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer()\n",
    "bert_wordpiece_tokenizer.train(\n",
    "    files = [small_corpus],\n",
    "    vocab_size = 10,\n",
    "    min_frequency = 1,\n",
    "    limit_alphabet = 1000,\n",
    "    initial_alphabet = [],\n",
    "    special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    show_progress = True,\n",
    "    wordpieces_prefix = \"##\",\n",
    ")\n",
    "\n",
    "vocab = bert_wordpiece_tokenizer.get_vocab()\n",
    "\n",
    "print(sorted(vocab, key=lambda x: vocab[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vX_FJCd1u2e_",
    "outputId": "7aa79a90-0cd6-4d87-e293-5813c5f9dbf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', '##b', '##c', '##d', '##e']\n",
      "[5, 12, 11, 13, 14]\n"
     ]
    }
   ],
   "source": [
    "encoding = bert_wordpiece_tokenizer.encode('ABCDE')\n",
    "\n",
    "print(encoding.tokens)\n",
    "print(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5zFqu6Ffu2fG",
    "outputId": "81681ee0-9c1a-42b2-9b59-b4cd3d15aba2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', 'a', 'b', 'c', 'd', 'e', 'f', 'g', '##f', '##c', '##b', '##d', '##e', 'ab', 'abc', 'af']\n"
     ]
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer()\n",
    "bert_wordpiece_tokenizer.train(\n",
    "    files = [small_corpus],\n",
    "    vocab_size = 20,\n",
    "    min_frequency = 1,\n",
    "    initial_alphabet = ['g'],\n",
    ")\n",
    "\n",
    "vocab = bert_wordpiece_tokenizer.get_vocab()\n",
    "\n",
    "print(sorted(vocab, key=lambda x: vocab[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "P6iZR9Qnu2fM",
    "outputId": "231864d1-4ded-4848-f5bd-4a7c0dfff349"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', '##d', '##e']\n",
      "[18, 15, 16]\n"
     ]
    }
   ],
   "source": [
    "encoding = bert_wordpiece_tokenizer.encode('ABCDE')\n",
    "\n",
    "print(encoding.tokens)\n",
    "print(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "a6lTaaj-u2fg",
    "outputId": "a4ee8a68-ad83-41c8-822b-81548ebc1329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', '##d', '##e']\n",
      "['abc', '##d']\n"
     ]
    }
   ],
   "source": [
    "encodings = bert_wordpiece_tokenizer.encode_batch(['ABCDE', 'abcd'])\n",
    "\n",
    "print(encodings[0].tokens)\n",
    "print(encodings[1].tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "g2jarhkBu2fm",
    "outputId": "0efe633b-dd23-412a-8baf-8b620d463046"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/very_small_bertwordpiece-vocab.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer.save_model(\n",
    "    directory = './model/',\n",
    "    name = 'very_small_bertwordpiece'\n",
    ")\n",
    "# ['./very_small_bertwordpiece-vocab.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1ErYaikOu2ft",
    "outputId": "469dc4ba-96b0-4355-8ab9-63d36515022d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'abc', '##d', '##e', '[SEP]']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer(\n",
    "    vocab_file = './model/very_small_bertwordpiece-vocab.txt'\n",
    ")\n",
    "bert_wordpiece_tokenizer.encode('ABCDE').tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Yv94ALVou2f3",
    "outputId": "8324095c-44a8-4b8a-d655-6460c279d3de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc', '##d', '##e']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer.encode('ABCDE', add_special_tokens=False).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "F_Bmj9V1u2gF",
    "outputId": "612a02f2-94f9-4415-854d-dd3ba83169e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'abc', '##d', '##e', '[SEP]', 'abc', '##d', '[SEP]']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer.encode(\n",
    "    sequence = 'abcde',\n",
    "    pair = 'abcd'\n",
    ").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "PJAb8VYju2gK",
    "outputId": "9ae94d94-daaa-4ffb-a0a8-0d3c3976a474"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', 'a', 'b', 'c', 'd', 'e', 'f', 'g', '##f', '##c', '##b', '##d', '##e', 'ab', 'abc', 'af']\n"
     ]
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer.add_tokens(['lovit'])\n",
    "\n",
    "bert_wordpiece_tokenizer.save_model(\n",
    "    directory = './model/',\n",
    "    name = 'very_small_bertwordpiece_lovit')\n",
    "\n",
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer(\n",
    "    vocab_file = './model/very_small_bertwordpiece_lovit-vocab.txt'\n",
    ")\n",
    "\n",
    "vocab = bert_wordpiece_tokenizer.get_vocab()\n",
    "\n",
    "print(sorted(vocab, key=lambda x: vocab[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "aRucaVBSu2gQ",
    "outputId": "bbbb085e-8eef-49c6-c4f4-fc0b640938bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'abc', '##d', '##e', '[UNK]', 'lovit', '[SEP]']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer(vocab_file='./model/very_small_bertwordpiece_lovit2-vocab.txt')\n",
    "\n",
    "bert_wordpiece_tokenizer.encode('ABCDE abg lovit').tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XiestA8u2gW"
   },
   "source": [
    "## SentencePiece BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9sgcYKQvu2gX",
    "outputId": "812f93d4-8b1c-4a28-cdbd-f1513d9679d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', 'A', 'B', 'C', 'D', 'E', 'F', '▁', '▁A', '▁AB', '▁ABC', 'DE', '▁DE', '▁AC', '▁AF', '▁ABD', '▁ABCDE']\n"
     ]
    }
   ],
   "source": [
    "sentencepiece_tokenizer = SentencePieceBPETokenizer(add_prefix_space = True)\n",
    "sentencepiece_tokenizer.train(\n",
    "    files = [small_corpus],\n",
    "    vocab_size = 20,\n",
    "    min_frequency = 1,\n",
    "    special_tokens = ['<unk>'],\n",
    ")\n",
    "\n",
    "vocab = sentencepiece_tokenizer.get_vocab()\n",
    "\n",
    "print(sorted(vocab, key=lambda x: vocab[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_Y20DlCAu2gf",
    "outputId": "490d3552-d586-48e7-f79f-68d34f3ea42a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', 'lovit', 'A', 'B', 'C', 'D', 'E', 'F', '▁', '▁A', '▁AB', 'DE', '▁ABC', 'AB', 'CDE', '▁AC', '▁AF', '▁ABD', 'ABCDE']\n"
     ]
    }
   ],
   "source": [
    "sentencepiece_tokenizer = SentencePieceBPETokenizer(\n",
    "    add_prefix_space = False\n",
    ")\n",
    "\n",
    "sentencepiece_tokenizer.train(\n",
    "    files = [small_corpus],\n",
    "    vocab_size = 20,\n",
    "    min_frequency = 1,\n",
    "    special_tokens = ['<unk>', 'lovit'],\n",
    ")\n",
    "\n",
    "vocab = sentencepiece_tokenizer.get_vocab()\n",
    "\n",
    "print(sorted(vocab, key=lambda x: vocab[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "VRRlxDx9u2gn",
    "outputId": "a5baa0a0-3bb7-4af5-9d18-bfd629335415"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/very_small_sentencepiece-vocab.json',\n",
       " './model/very_small_sentencepiece-merges.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentencepiece_tokenizer.save_model('./model/', 'very_small_sentencepiece')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "tj4CQoDeu2gs",
    "outputId": "65578738-0a22-4cc1-c6db-c4fefa83b857"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁ABC', 'DE']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentencepiece_tokenizer = SentencePieceBPETokenizer(\n",
    "    vocab_file = './model/very_small_sentencepiece-vocab.json',\n",
    "    merges_file = './model/very_small_sentencepiece-merges.txt'\n",
    ")\n",
    "\n",
    "sentencepiece_tokenizer.encode('ABCDE').tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5nZLpBZu2g4"
   },
   "source": [
    "## Character BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "MWh5IYsuu2g5",
    "outputId": "a8ab49fa-845f-4780-a2a7-16ac273f7f2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AB', 'C', 'DE</w>', 'ABC</w>']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charbpe_tokenizer = CharBPETokenizer(suffix='</w>')\n",
    "charbpe_tokenizer.train(\n",
    "    files = [small_corpus],\n",
    "    vocab_size = 15,\n",
    "    min_frequency = 1\n",
    ")\n",
    "charbpe_tokenizer.encode('ABCDE.ABC').tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "pf2WVJiCu2g-",
    "outputId": "e12382f9-d5b2-4c33-e6c2-0be5ba9dd853"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AB', 'C', 'D', 'E', 'ABC</w>']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charbpe_tokenizer = CharBPETokenizer(\n",
    "    suffix='</w>',\n",
    "    split_on_whitespace_only = True\n",
    ")\n",
    "charbpe_tokenizer.train(\n",
    "    files = [small_corpus],\n",
    "    vocab_size = 15,\n",
    "    min_frequency = 1\n",
    ")\n",
    "charbpe_tokenizer.encode('ABCDE.ABC').tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "eNfJHxBdu2hC"
   },
   "outputs": [],
   "source": [
    "charbpe_tokenizer.train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "uZWT42fGu2hL",
    "outputId": "a83e45ab-16da-4d32-ff26-3ce243654196"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AB', 'C', 'D', 'E', 'ABC</w>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charbpe_tokenizer = CharBPETokenizer(\n",
    "    suffix='</w>',\n",
    "    split_on_whitespace_only = True\n",
    ")\n",
    "charbpe_tokenizer.train(\n",
    "    files = [small_corpus],\n",
    "    vocab_size = 15,\n",
    "    min_frequency = 1,\n",
    ")\n",
    "charbpe_tokenizer.encode('ABCDE.ABC').tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Vo88xnKHu2hT",
    "outputId": "a39bcc9f-6fcc-46c5-9b14-0144f0eb98ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AB', 'C', 'D', 'E', 'F']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charbpe_tokenizer.encode('ABCDEFGH').tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5CWk3M3u2ha"
   },
   "source": [
    "## Byte-level BPE Tokenizer\n",
    "\n",
    "- OpenAI GPT2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ZgZ4EfT7u2hc",
    "outputId": "13d4dcab-9b22-4ed6-8fa2-818d51eb8197"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '¡', '¢', '£', '¤', '¥', '¦', '§', '¨', '©', 'ª', '«', '¬', '®', '¯', '°', '±', '²', '³', '´', 'µ', '¶', '·', '¸', '¹', 'º', '»', '¼', '½', '¾', '¿', 'À', 'Á', 'Â', 'Ã', 'Ä', 'Å', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ï', 'Ð', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', '×', 'Ø', 'Ù', 'Ú', 'Û', 'Ü', 'Ý', 'Þ', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', '÷', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'þ', 'ÿ', 'Ā', 'ā', 'Ă', 'ă', 'Ą', 'ą', 'Ć', 'ć', 'Ĉ', 'ĉ', 'Ċ', 'ċ', 'Č', 'č', 'Ď', 'ď', 'Đ', 'đ', 'Ē', 'ē', 'Ĕ', 'ĕ', 'Ė', 'ė', 'Ę', 'ę', 'Ě', 'ě', 'Ĝ', 'ĝ', 'Ğ', 'ğ', 'Ġ', 'ġ', 'Ģ', 'ģ', 'Ĥ', 'ĥ', 'Ħ', 'ħ', 'Ĩ', 'ĩ', 'Ī', 'ī', 'Ĭ', 'ĭ', 'Į', 'į', 'İ', 'ı', 'Ĳ', 'ĳ', 'Ĵ', 'ĵ', 'Ķ', 'ķ', 'ĸ', 'Ĺ', 'ĺ', 'Ļ', 'ļ', 'Ľ', 'ľ', 'Ŀ', 'ŀ', 'Ł', 'ł', 'Ń', 'ĠA', 'ĠAB', 'ĠABC', 'DE', 'ĠDE', 'ĠAC', 'ĠAF', 'ĠABD', 'ĠABCDE']\n"
     ]
    }
   ],
   "source": [
    "bytebpe_tokenizer = ByteLevelBPETokenizer(add_prefix_space=True)\n",
    "bytebpe_tokenizer.train(files = [small_corpus],\n",
    "    vocab_size = 1000, min_frequency = 1)\n",
    "vocab = bytebpe_tokenizer.get_vocab()\n",
    "print(sorted(vocab, key=lambda x: vocab[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "k5nitw70u2hi",
    "outputId": "2a777377-680f-49c7-b2f7-40ffefd829a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ĠABCDE', 'ĠABC']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytebpe_tokenizer.encode('ABCDE ABC').tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GM6qwG3_u2ho"
   },
   "source": [
    "## 코로나19 관련 뉴스를 학습해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./model/ByteLevelBPETokenizer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "rh9iIm4mu2hq",
    "outputId": "38b9ea17-5da5-4ba8-fe46-1dc163b71629"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/ByteLevelBPETokenizer/covid-vocab.json',\n",
       " './model/ByteLevelBPETokenizer/covid-merges.txt']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import (ByteLevelBPETokenizer,\n",
    "                        CharBPETokenizer,\n",
    "                        SentencePieceBPETokenizer,\n",
    "                        BertWordPieceTokenizer)\n",
    "\n",
    "corpus_path = './data/2020-07-29_covid_news_sents.txt'\n",
    "vocab_size = 3000\n",
    "\n",
    "byte_level_bpe_tokenizer = ByteLevelBPETokenizer()\n",
    "byte_level_bpe_tokenizer.train(files=[corpus_path], vocab_size=vocab_size)\n",
    "byte_level_bpe_tokenizer.save_model(directory='./model/ByteLevelBPETokenizer/', name='covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./model/CharBPETokenizer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "hcB1h4Nzu2h_",
    "outputId": "9e224643-20a7-4654-d715-dfded28de232"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/CharBPETokenizer/covid-vocab.json',\n",
       " './model/CharBPETokenizer/covid-merges.txt']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_bpe_tokenizer = CharBPETokenizer()\n",
    "char_bpe_tokenizer.train(files=[corpus_path], vocab_size=vocab_size)\n",
    "char_bpe_tokenizer.save_model(directory='./model/CharBPETokenizer/', name='covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./model/SentencePieceBPETokenizer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "DfeI9AA-u2iE",
    "outputId": "cf827e48-eb84-40d0-db2b-97fb950ee519"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/SentencePieceBPETokenizer/covid-vocab.json',\n",
       " './model/SentencePieceBPETokenizer/covid-merges.txt']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentencepiece_bpe_tokenizer = SentencePieceBPETokenizer()\n",
    "sentencepiece_bpe_tokenizer.train(files=[corpus_path], vocab_size=vocab_size)\n",
    "sentencepiece_bpe_tokenizer.save_model(directory='./model/SentencePieceBPETokenizer/', name='covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./model/BertWordPieceTokenizer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "3glclcQvu2iJ",
    "outputId": "6661f13d-24b4-4e56-8572-02d87aa5f5ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/BertWordPieceTokenizer/covid-vocab.txt']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer()\n",
    "\n",
    "bert_wordpiece_tokenizer.train(\n",
    "    files=[corpus_path], vocab_size=vocab_size)\n",
    "\n",
    "bert_wordpiece_tokenizer.save_model(\n",
    "    directory='./model/BertWordPieceTokenizer/', name='covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "VVgp8OFSu2iO",
    "outputId": "e641a5ba-6c2a-48f3-8acc-fdf7eb8c2831"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BertWordPieceTokenizer\n",
      "tokens = ['신종', '코로나바이러스', '감염증', '(', '코로나19', ')', '사태', '##가', '심', '##각', '##합니다']\n",
      "\n",
      "SentencePieceBPETokenizer\n",
      "tokens = ['▁신종', '▁코로나바이러스', '▁감염증(코로나19)', '▁사태', '가', '▁심', '각', '합', '니다']\n",
      "\n",
      "CharBPETokenizer\n",
      "tokens = ['신종</w>', '코로나바이러스</w>', '감염증</w>', '(</w>', '코로나19</w>', ')</w>', '사태', '가</w>', '심', '각', '합니다</w>']\n",
      "\n",
      "ByteLevelBPETokenizer\n",
      "tokens = ['ìĭłì¢ħ', 'Ġì½Ķë¡ľëĤĺë°ĶìĿ´ëŁ¬ìĬ¤', 'Ġê°ĲìĹ¼ì¦Ŀ', '(', 'ì½Ķë¡ľëĤĺ', '19', ')', 'ĠìĤ¬íĥľ', 'ê°Ģ', 'Ġìĭ¬', 'ê°ģ', 'íķ©ëĭĪëĭ¤']\n"
     ]
    }
   ],
   "source": [
    "sent_ko = '신종 코로나바이러스 감염증(코로나19) 사태가 심각합니다'\n",
    "tokenizers = [bert_wordpiece_tokenizer,\n",
    "              sentencepiece_bpe_tokenizer,\n",
    "              char_bpe_tokenizer,\n",
    "              byte_level_bpe_tokenizer]\n",
    "\n",
    "for tokenizer in tokenizers:\n",
    "    encode_single = tokenizer.encode(sent_ko)\n",
    "    print(f'\\n{tokenizer.__class__.__name__}')\n",
    "    print(f'tokens = {encode_single.tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vylxbWTFu2if"
   },
   "source": [
    "## 학습한 토크나이저를 transformers 에서 이용하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "wYDwBa_9u2ig",
    "outputId": "bc7a30fb-7f2e-4207-8c97-adf7d388ae4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers  : ['신종', '코로나바이러스', '감염증', '(', '코로나19', ')', '사태', '##가', '심', '##각', '##합니다']\n",
      "transformers: ['신종', '코로나바이러스', '감염증', '(', '코로나19', ')', '사태', '##가', '심', '##각', '##합니다']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, GPT2Tokenizer\n",
    "\n",
    "transformers_bert_tokenizer = BertTokenizer(\n",
    "    vocab_file = './model/BertWordPieceTokenizer/covid-vocab.txt'\n",
    ")\n",
    "\n",
    "print(f'tokenizers  : {bert_wordpiece_tokenizer.encode(sent_ko).tokens}')\n",
    "print(f'transformers: {transformers_bert_tokenizer.tokenize(sent_ko)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "_7DHY3Tmu2ik",
    "outputId": "6d1e919e-0676-4dd6-e1fa-085ef9282767"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(신종) = 6\n",
      "len(코로나바이러스) = 14\n",
      "len(감염증) = 9\n"
     ]
    }
   ],
   "source": [
    "for token in bert_wordpiece_tokenizer.encode(sent_ko).tokens[:3]:\n",
    "    print(f'len({token}) = {len(token)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "zPVhaSzwu2in",
    "outputId": "696a9d23-d1de-409a-edd1-2d544b657e35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가감\n",
      "5\n",
      "가감\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "print(normalize('NFKD', '가감'))  # 출력 시 글자를 재조합해서 보여줌\n",
    "print(len(normalize('NFKD', '가감')))\n",
    "print(normalize('NFKC', normalize('NFKD', '가감')))\n",
    "print(len(normalize('NFKC', normalize('NFKD', '가감'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "5gw74mW5u2is",
    "outputId": "09a78147-8961-4306-e82d-ae00331b431d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers  : ['신종', '코로나바이러스', '감염증', '(', '코로나19', ')', '사태', '##가', '심', '##각', '##합니다']\n",
      "transformers: ['신종', '코로나바이러스', '감염증', '(', '코로나19', ')', '사태', '##가', '심', '##각', '##합니다']\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "def compose(tokens):\n",
    "    return [normalize('NFKC', token) for token in tokens]\n",
    "\n",
    "print(f'tokenizers  : {compose(bert_wordpiece_tokenizer.encode(sent_ko).tokens)}')\n",
    "print(f'transformers: {compose(transformers_bert_tokenizer.tokenize(sent_ko))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "i_RdLPD0u2iy",
    "outputId": "78906c3d-42b3-48a9-e58d-62446223ae2e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers  : ['ìĭłì¢ħ', 'Ġì½Ķë¡ľëĤĺë°ĶìĿ´ëŁ¬ìĬ¤', 'Ġê°ĲìĹ¼ì¦Ŀ', '(', 'ì½Ķë¡ľëĤĺ', '19', ')', 'ĠìĤ¬íĥľ', 'ê°Ģ', 'Ġìĭ¬', 'ê°ģ', 'íķ©ëĭĪëĭ¤']\n",
      "transformers: ['ìĭłì¢ħ', 'Ġì½Ķë¡ľëĤĺë°ĶìĿ´ëŁ¬ìĬ¤', 'Ġê°ĲìĹ¼ì¦Ŀ', '(', 'ì½Ķë¡ľëĤĺ', '19', ')', 'ĠìĤ¬íĥľ', 'ê°Ģ', 'Ġìĭ¬', 'ê°ģ', 'íķ©ëĭĪëĭ¤']\n"
     ]
    }
   ],
   "source": [
    "transformers_gpt2_tokenizer = GPT2Tokenizer(\n",
    "    vocab_file = './model/ByteLevelBPETokenizer/covid-vocab.json',\n",
    "    merges_file = './model/ByteLevelBPETokenizer/covid-merges.txt'\n",
    ")\n",
    "\n",
    "print(f'tokenizers  : {byte_level_bpe_tokenizer.encode(sent_ko).tokens}')\n",
    "print(f'transformers: {transformers_gpt2_tokenizer.tokenize(sent_ko)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGn_ErAlu2i2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "-XiestA8u2gW",
    "P5nZLpBZu2g4",
    "Y5CWk3M3u2ha",
    "GM6qwG3_u2ho",
    "vylxbWTFu2if"
   ],
   "include_colab_link": true,
   "name": "00_huggingface_tokenizers_usage.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
