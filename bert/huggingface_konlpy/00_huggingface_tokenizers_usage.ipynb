{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "00_huggingface_tokenizers_usage.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-XiestA8u2gW",
        "P5nZLpBZu2g4",
        "Y5CWk3M3u2ha",
        "GM6qwG3_u2ho",
        "vylxbWTFu2if"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ejpark78/codelab/blob/master/bert/huggingface_konlpy/00_huggingface_tokenizers_usage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yptqj7Abv3H9"
      },
      "source": [
        "출처: https://github.com/lovit/huggingface_konlpy/blob/master/tutorials/00_huggingface_tokenizers_usage.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbTTfXCsvBWB",
        "outputId": "654a6026-68c1-4171-edb2-aecad903e386",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!rm -rf sample_data\n",
        "\n",
        "# git clone\n",
        "!git init\n",
        "!git pull https://github.com/lovit/huggingface_konlpy.git\n",
        "!pip install -r requirements.txt\n",
        "!pip freeze | grep transformers\n",
        "\n",
        "# mecab-ko 설치\n",
        "!curl -s https://raw.githubusercontent.com/SOMJANG/Mecab-ko-for-Google-Colab/master/install_mecab-ko_on_colab190912.sh | bash"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized empty Git repository in /content/.git/\n",
            "remote: Enumerating objects: 195, done.\u001b[K\n",
            "remote: Counting objects: 100% (195/195), done.\u001b[K\n",
            "remote: Compressing objects: 100% (125/125), done.\u001b[K\n",
            "remote: Total 195 (delta 78), reused 170 (delta 62), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (195/195), 5.44 MiB | 18.63 MiB/s, done.\n",
            "Resolving deltas: 100% (78/78), done.\n",
            "From https://github.com/lovit/huggingface_konlpy\n",
            " * branch            HEAD       -> FETCH_HEAD\n",
            "Collecting konlpy>=0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.3MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ee/fedc3509145ad60fe5b418783f4a4c1b5462a4f0e8c7bbdbda52bdcda486/tokenizers-0.8.1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 42.6MB/s \n",
            "\u001b[?25hCollecting transformers==3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 40.9MB/s \n",
            "\u001b[?25hCollecting tqdm>=4.46.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/0e/ea53a3d6f1eb2cc31162c9ae89555cc26a3986e5559781f0b0df75aea5cf/tqdm-4.50.0-py2.py3-none-any.whl (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.3MB/s \n",
            "\u001b[?25hCollecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/4a/0f98dd46ba212ff8c40a430b5dc884c15fddd256a875223dd53d90eca1af/wandb-0.10.4-py2.py3-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 39.2MB/s \n",
            "\u001b[?25hCollecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy>=0.5.2->-r requirements.txt (line 1)) (1.18.5)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/f7/a368401e630f0e390dd0e62c39fb928e5b23741b53c2360ee7d376660927/JPype1-1.0.2-cp36-cp36m-manylinux2010_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 43.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy>=0.5.2->-r requirements.txt (line 1)) (4.2.6)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.3MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 36.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->-r requirements.txt (line 3)) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->-r requirements.txt (line 3)) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->-r requirements.txt (line 3)) (2.23.0)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 38.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->-r requirements.txt (line 3)) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->-r requirements.txt (line 3)) (20.4)\n",
            "Collecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/06/121302598a4fc01aca942d937f4a2c33430b7181137b35758913a8db10ad/watchdog-0.10.3.tar.gz (94kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from wandb->-r requirements.txt (line 5)) (2.3)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/d7/b2b0672e0331567157adf9281f41ee731c412ee518ca5e6552c27fa73c91/GitPython-3.1.9-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 50.6MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from wandb->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from wandb->-r requirements.txt (line 5)) (3.13)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb->-r requirements.txt (line 5)) (7.1.2)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb->-r requirements.txt (line 5)) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb->-r requirements.txt (line 5)) (2.8.1)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/df/1145dc9389138eb47649806b42aaad5b0ecdfd3e93c7c51c1fffd80a8f90/sentry_sdk-0.18.0-py2.py3-none-any.whl (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 52.3MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 12.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.6/dist-packages (from wandb->-r requirements.txt (line 5)) (3.12.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy>=0.5.2->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy>=0.5.2->-r requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2->-r requirements.txt (line 3)) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2->-r requirements.txt (line 3)) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2->-r requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2->-r requirements.txt (line 3)) (2.4.7)\n",
            "Collecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.12.0->wandb->-r requirements.txt (line 5)) (50.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy>=0.5.2->-r requirements.txt (line 1)) (3.1.0)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: sacremoses, watchdog, subprocess32, pathtools\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=d9b4f6ad9e5534d9d2dad66f9c061e11b75928a414a451dd74d48befe7e2ee1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.3-cp36-none-any.whl size=73873 sha256=e9df30d8e17db80be4fb4bf0ed9f43fa2f9a9c6b9430f9c6c806aa7bb25e7906\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/1d/38/2c19bb311f67cc7b4d07a2ec5ea36ab1a0a0ea50db994a5bc7\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=8d478b375199c254be2d4e1017ff4ed24341bcae61b9eb71e8493cc576f9596e\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8785 sha256=c8630410cff1dcbfc05fcff84fc2219a325be4debfec01563fafef4cb483ccdb\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built sacremoses watchdog subprocess32 pathtools\n",
            "\u001b[31mERROR: transformers 3.0.2 has requirement tokenizers==0.8.1.rc1, but you'll have tokenizers 0.8.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tweepy, colorama, JPype1, beautifulsoup4, konlpy, tokenizers, tqdm, sacremoses, sentencepiece, transformers, pathtools, watchdog, smmap, gitdb, GitPython, shortuuid, configparser, docker-pycreds, sentry-sdk, subprocess32, wandb\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed GitPython-3.1.9 JPype1-1.0.2 beautifulsoup4-4.6.0 colorama-0.4.3 configparser-5.0.0 docker-pycreds-0.4.0 gitdb-4.0.5 konlpy-0.5.2 pathtools-0.1.2 sacremoses-0.0.43 sentencepiece-0.1.91 sentry-sdk-0.18.0 shortuuid-1.0.1 smmap-3.0.4 subprocess32-3.5.4 tokenizers-0.8.1 tqdm-4.50.0 transformers-3.0.2 tweepy-3.9.0 wandb-0.10.4 watchdog-0.10.3\n",
            "transformers==3.0.2\n",
            "Installing konlpy.....\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.0.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (3.9.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Done\n",
            "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
            "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "--2020-10-01 10:38:43--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 18.205.93.1, 18.205.93.2, 18.205.93.0, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|18.205.93.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=sVPkKqxxG3i9LYzoDurTMOxkAS4%3D&Expires=1601549565&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None [following]\n",
            "--2020-10-01 10:38:43--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=sVPkKqxxG3i9LYzoDurTMOxkAS4%3D&Expires=1601549565&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.104.60\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.104.60|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1414979 (1.3M) [application/x-tar]\n",
            "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’\n",
            "\n",
            "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2020-10-01 10:38:44 (16.3 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]\n",
            "\n",
            "Done\n",
            "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-0.996-ko-0.9.2.......\n",
            "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
            "configure\n",
            "make\n",
            "make check\n",
            "make install\n",
            "ldconfig\n",
            "Done\n",
            "Change Directory to /content\n",
            "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "--2020-10-01 10:40:12--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 18.205.93.0, 18.205.93.1, 18.205.93.2, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|18.205.93.0|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=LVH0r31xmnQfNGK7x34xLLf81MU%3D&Expires=1601549983&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None [following]\n",
            "--2020-10-01 10:40:12--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=LVH0r31xmnQfNGK7x34xLLf81MU%3D&Expires=1601549983&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.230.155\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.230.155|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49775061 (47M) [application/x-tar]\n",
            "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’\n",
            "\n",
            "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M   107MB/s    in 0.4s    \n",
            "\n",
            "2020-10-01 10:40:12 (107 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]\n",
            "\n",
            "Done\n",
            "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
            "Done\n",
            "installing........\n",
            "configure\n",
            "make\n",
            "make install\n",
            "apt-get update\n",
            "apt-get upgrade\n",
            "apt install curl\n",
            "apt install git\n",
            "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
            "Done\n",
            "Successfully Installed\n",
            "Now you can use Mecab\n",
            "from konlpy.tag import Mecab\n",
            "mecab = Mecab()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92R0sf6lu2eO",
        "outputId": "37ffa4f2-5c35-41a1-9898-277ae8fa5e0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tokenizers\n",
        "tokenizers.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.8.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C43_IkS-u2eb"
      },
      "source": [
        "from tokenizers import (ByteLevelBPETokenizer,\n",
        "                        CharBPETokenizer,\n",
        "                        SentencePieceBPETokenizer,\n",
        "                        BertWordPieceTokenizer)\n",
        "\n",
        "small_corpus = 'very_small_corpus.txt'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlOX6Y4nu2ej"
      },
      "source": [
        "## Bert WordPiece Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44hkKpHqu2eo"
      },
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "bert_wordpiece_tokenizer = BertWordPieceTokenizer(\n",
        "    lowercase = False)\n",
        "bert_wordpiece_tokenizer.train(files=[small_corpus], vocab_size=10)\n",
        "encoding = bert_wordpiece_tokenizer.encode('ABCDE')\n",
        "print(encoding.tokens)\n",
        "print(encoding.ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__tG4vIxu2e1",
        "outputId": "5625a61f-f0f6-4644-d207-c293abf82775"
      },
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "bert_wordpiece_tokenizer = BertWordPieceTokenizer()\n",
        "bert_wordpiece_tokenizer.train(\n",
        "    files = [small_corpus],\n",
        "    vocab_size = 10,\n",
        "    min_frequency = 1,\n",
        "    limit_alphabet = 1000,\n",
        "    initial_alphabet = [],\n",
        "    special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
        "    show_progress = True,\n",
        "    wordpieces_prefix = \"##\",\n",
        ")\n",
        "vocab = bert_wordpiece_tokenizer.get_vocab()\n",
        "print(sorted(vocab, key=lambda x: vocab[x]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', 'a', 'b', 'c', 'd', 'e', 'f', '##f', '##b', '##c', '##d', '##e']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX_FJCd1u2e_",
        "outputId": "7aa79a90-0cd6-4d87-e293-5813c5f9dbf8"
      },
      "source": [
        "encoding = bert_wordpiece_tokenizer.encode('ABCDE')\n",
        "print(encoding.tokens)\n",
        "print(encoding.ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['a', '##b', '##c', '##d', '##e']\n",
            "[5, 12, 13, 14, 15]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zFqu6Ffu2fG",
        "outputId": "81681ee0-9c1a-42b2-9b59-b4cd3d15aba2"
      },
      "source": [
        "bert_wordpiece_tokenizer = BertWordPieceTokenizer()\n",
        "bert_wordpiece_tokenizer.train(\n",
        "    files = [small_corpus],\n",
        "    vocab_size = 20,\n",
        "    min_frequency = 1,\n",
        "    initial_alphabet = ['g'],\n",
        ")\n",
        "vocab = bert_wordpiece_tokenizer.get_vocab()\n",
        "print(sorted(vocab, key=lambda x: vocab[x]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', 'a', 'b', 'c', 'd', 'e', 'f', 'g', '##c', '##b', '##d', '##e', '##f', 'ab', 'abc', 'ac']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6iZR9Qnu2fM",
        "outputId": "231864d1-4ded-4848-f5bd-4a7c0dfff349"
      },
      "source": [
        "encoding = bert_wordpiece_tokenizer.encode('ABCDE')\n",
        "print(encoding.tokens)\n",
        "print(encoding.ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['abc', '##d', '##e']\n",
            "[18, 14, 15]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6lTaaj-u2fg",
        "outputId": "a4ee8a68-ad83-41c8-822b-81548ebc1329"
      },
      "source": [
        "encodings = bert_wordpiece_tokenizer.encode_batch(['ABCDE', 'abcd'])\n",
        "print(encodings[0].tokens)\n",
        "print(encodings[1].tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['abc', '##d', '##e']\n",
            "['abc', '##d']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2jarhkBu2fm",
        "outputId": "0efe633b-dd23-412a-8baf-8b620d463046"
      },
      "source": [
        "bert_wordpiece_tokenizer.save_model(\n",
        "    directory = './',\n",
        "    name = 'very_small_bertwordpiece'\n",
        ")\n",
        "# ['./very_small_bertwordpiece-vocab.txt']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./very_small_bertwordpiece-vocab.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ErYaikOu2ft",
        "outputId": "469dc4ba-96b0-4355-8ab9-63d36515022d"
      },
      "source": [
        "bert_wordpiece_tokenizer = BertWordPieceTokenizer(\n",
        "    vocab_file = './very_small_bertwordpiece-vocab.txt'\n",
        ")\n",
        "bert_wordpiece_tokenizer.encode('ABCDE').tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]', 'abc', '##d', '##e', '[SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv94ALVou2f3",
        "outputId": "8324095c-44a8-4b8a-d655-6460c279d3de"
      },
      "source": [
        "bert_wordpiece_tokenizer.encode('ABCDE', add_special_tokens=False).tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['abc', '##d', '##e']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_Bmj9V1u2gF",
        "outputId": "612a02f2-94f9-4415-854d-dd3ba83169e2"
      },
      "source": [
        "bert_wordpiece_tokenizer.encode(\n",
        "    sequence = 'abcde',\n",
        "    pair = 'abcd'\n",
        ").tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]', 'abc', '##d', '##e', '[SEP]', 'abc', '##d', '[SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJAb8VYju2gK",
        "outputId": "9ae94d94-daaa-4ffb-a0a8-0d3c3976a474"
      },
      "source": [
        "bert_wordpiece_tokenizer.add_tokens(['lovit'])\n",
        "bert_wordpiece_tokenizer.save_model(\n",
        "    directory = './',\n",
        "    name = 'very_small_bertwordpiece_lovit')\n",
        "bert_wordpiece_tokenizer = BertWordPieceTokenizer(\n",
        "    vocab_file = './very_small_bertwordpiece_lovit-vocab.txt'\n",
        ")\n",
        "vocab = bert_wordpiece_tokenizer.get_vocab()\n",
        "print(sorted(vocab, key=lambda x: vocab[x]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', 'a', 'b', 'c', 'd', 'e', 'f', 'g', '##c', '##b', '##d', '##e', '##f', 'ab', 'abc', 'ac']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRucaVBSu2gQ",
        "outputId": "bbbb085e-8eef-49c6-c4f4-fc0b640938bc"
      },
      "source": [
        "bert_wordpiece_tokenizer = BertWordPieceTokenizer(\n",
        "    vocab_file = './very_small_bertwordpiece_lovit2-vocab.txt')\n",
        "bert_wordpiece_tokenizer.encode('ABCDE abg lovit').tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]', 'abcde', '[UNK]', 'lovit', '[SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XiestA8u2gW"
      },
      "source": [
        "## SentencePiece BPE Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sgcYKQvu2gX",
        "outputId": "812f93d4-8b1c-4a28-cdbd-f1513d9679d9"
      },
      "source": [
        "sentencepiece_tokenizer = SentencePieceBPETokenizer(\n",
        "    add_prefix_space = True)\n",
        "sentencepiece_tokenizer.train(\n",
        "    files = [small_corpus],\n",
        "    vocab_size = 20,\n",
        "    min_frequency = 1,\n",
        "    special_tokens = ['<unk>'],)\n",
        "vocab = sentencepiece_tokenizer.get_vocab()\n",
        "print(sorted(vocab, key=lambda x: vocab[x]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', 'A', 'B', 'C', 'D', 'E', 'F', '▁', '▁A', '▁AB', '▁ABC', 'DE', '▁DE', '▁AC', '▁AF', '▁ABD', '▁ABCDE']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y20DlCAu2gf",
        "outputId": "490d3552-d586-48e7-f79f-68d34f3ea42a"
      },
      "source": [
        "sentencepiece_tokenizer = SentencePieceBPETokenizer(\n",
        "    add_prefix_space = False\n",
        ")\n",
        "sentencepiece_tokenizer.train(\n",
        "    files = [small_corpus],\n",
        "    vocab_size = 20,\n",
        "    min_frequency = 1,\n",
        "    special_tokens = ['<unk>', 'lovit'],\n",
        ")\n",
        "vocab = sentencepiece_tokenizer.get_vocab()\n",
        "print(sorted(vocab, key=lambda x: vocab[x]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', 'lovit', 'A', 'B', 'C', 'D', 'E', 'F', '▁', '▁A', '▁AB', 'DE', '▁ABC', 'AB', 'CDE', '▁AC', '▁AF', '▁ABD', 'ABCDE']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRRlxDx9u2gn",
        "outputId": "a5baa0a0-3bb7-4af5-9d18-bfd629335415"
      },
      "source": [
        "sentencepiece_tokenizer.save_model('./', 'very_small_sentencepiece')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./very_small_sentencepiece-vocab.json',\n",
              " './very_small_sentencepiece-merges.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj4CQoDeu2gs",
        "outputId": "65578738-0a22-4cc1-c6db-c4fefa83b857"
      },
      "source": [
        "sentencepiece_tokenizer = SentencePieceBPETokenizer(\n",
        "    vocab_file = './very_small_sentencepiece-vocab.json',\n",
        "    merges_file = './very_small_sentencepiece-merges.txt'\n",
        ")\n",
        "sentencepiece_tokenizer.encode('ABCDE').tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁ABC', 'DE']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5nZLpBZu2g4"
      },
      "source": [
        "## Character BPE Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWh5IYsuu2g5",
        "outputId": "a8ab49fa-845f-4780-a2a7-16ac273f7f2c"
      },
      "source": [
        "charbpe_tokenizer = CharBPETokenizer(suffix='</w>')\n",
        "charbpe_tokenizer.train(\n",
        "    files = [small_corpus],\n",
        "    vocab_size = 15,\n",
        "    min_frequency = 1\n",
        ")\n",
        "charbpe_tokenizer.encode('ABCDE.ABC').tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['AB', 'C', 'DE</w>', 'ABC</w>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf2WVJiCu2g-",
        "outputId": "e12382f9-d5b2-4c33-e6c2-0be5ba9dd853"
      },
      "source": [
        "charbpe_tokenizer = CharBPETokenizer(\n",
        "    suffix='</w>',\n",
        "    split_on_whitespace_only = True\n",
        ")\n",
        "charbpe_tokenizer.train(\n",
        "    files = [small_corpus],\n",
        "    vocab_size = 15,\n",
        "    min_frequency = 1\n",
        ")\n",
        "charbpe_tokenizer.encode('ABCDE.ABC').tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['AB', 'C', 'D', 'E', 'ABC</w>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNfJHxBdu2hC"
      },
      "source": [
        "charbpe_tokenizer.train?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZWT42fGu2hL",
        "outputId": "a83e45ab-16da-4d32-ff26-3ce243654196"
      },
      "source": [
        "charbpe_tokenizer = CharBPETokenizer(\n",
        "    suffix='</w>',\n",
        "    split_on_whitespace_only = True\n",
        ")\n",
        "charbpe_tokenizer.train(\n",
        "    files = [small_corpus],\n",
        "    vocab_size = 15,\n",
        "    min_frequency = 1,\n",
        ")\n",
        "charbpe_tokenizer.encode('ABCDE.ABC').tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['AB', 'C', 'D', 'E', 'ABC</w>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo88xnKHu2hT",
        "outputId": "a39bcc9f-6fcc-46c5-9b14-0144f0eb98ba"
      },
      "source": [
        "charbpe_tokenizer.encode('ABCDEFGH').tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['AB', 'C', 'D', 'E', 'F']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5CWk3M3u2ha"
      },
      "source": [
        "## Byte-level BPE Tokenizer\n",
        "\n",
        "- OpenAI GPT2 tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgZ4EfT7u2hc",
        "outputId": "13d4dcab-9b22-4ed6-8fa2-818d51eb8197"
      },
      "source": [
        "bytebpe_tokenizer = ByteLevelBPETokenizer(add_prefix_space=True)\n",
        "bytebpe_tokenizer.train(files = [small_corpus],\n",
        "    vocab_size = 1000, min_frequency = 1)\n",
        "vocab = bytebpe_tokenizer.get_vocab()\n",
        "print(sorted(vocab, key=lambda x: vocab[x]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '¡', '¢', '£', '¤', '¥', '¦', '§', '¨', '©', 'ª', '«', '¬', '®', '¯', '°', '±', '²', '³', '´', 'µ', '¶', '·', '¸', '¹', 'º', '»', '¼', '½', '¾', '¿', 'À', 'Á', 'Â', 'Ã', 'Ä', 'Å', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ï', 'Ð', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', '×', 'Ø', 'Ù', 'Ú', 'Û', 'Ü', 'Ý', 'Þ', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', '÷', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'þ', 'ÿ', 'Ā', 'ā', 'Ă', 'ă', 'Ą', 'ą', 'Ć', 'ć', 'Ĉ', 'ĉ', 'Ċ', 'ċ', 'Č', 'č', 'Ď', 'ď', 'Đ', 'đ', 'Ē', 'ē', 'Ĕ', 'ĕ', 'Ė', 'ė', 'Ę', 'ę', 'Ě', 'ě', 'Ĝ', 'ĝ', 'Ğ', 'ğ', 'Ġ', 'ġ', 'Ģ', 'ģ', 'Ĥ', 'ĥ', 'Ħ', 'ħ', 'Ĩ', 'ĩ', 'Ī', 'ī', 'Ĭ', 'ĭ', 'Į', 'į', 'İ', 'ı', 'Ĳ', 'ĳ', 'Ĵ', 'ĵ', 'Ķ', 'ķ', 'ĸ', 'Ĺ', 'ĺ', 'Ļ', 'ļ', 'Ľ', 'ľ', 'Ŀ', 'ŀ', 'Ł', 'ł', 'Ń', 'ĠA', 'ĠAB', 'ĠABC', 'DE', 'ĠDE', 'ĠAC', 'ĠAF', 'ĠABD', 'ĠABCDE']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5nitw70u2hi",
        "outputId": "2a777377-680f-49c7-b2f7-40ffefd829a8"
      },
      "source": [
        "bytebpe_tokenizer.encode('ABCDE ABC').tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ĠABCDE', 'ĠABC']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM6qwG3_u2ho"
      },
      "source": [
        "## 코로나19 관련 뉴스를 학습해 보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh9iIm4mu2hq",
        "outputId": "38b9ea17-5da5-4ba8-fe46-1dc163b71629"
      },
      "source": [
        "from tokenizers import (ByteLevelBPETokenizer,\n",
        "                        CharBPETokenizer,\n",
        "                        SentencePieceBPETokenizer,\n",
        "                        BertWordPieceTokenizer)\n",
        "\n",
        "corpus_path = '../data/2020-07-29_covid_news_sents.txt'\n",
        "vocab_size = 3000\n",
        "\n",
        "byte_level_bpe_tokenizer = ByteLevelBPETokenizer()\n",
        "byte_level_bpe_tokenizer.train(files=[corpus_path], vocab_size=vocab_size)\n",
        "byte_level_bpe_tokenizer.save_model(directory='./tokenizers/ByteLevelBPETokenizer/', name='covid')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./tokenizers/ByteLevelBPETokenizer/covid-vocab.json',\n",
              " './tokenizers/ByteLevelBPETokenizer/covid-merges.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcB1h4Nzu2h_",
        "outputId": "9e224643-20a7-4654-d715-dfded28de232"
      },
      "source": [
        "char_bpe_tokenizer = CharBPETokenizer()\n",
        "char_bpe_tokenizer.train(files=[corpus_path], vocab_size=vocab_size)\n",
        "char_bpe_tokenizer.save_model(directory='./tokenizers/CharBPETokenizer/', name='covid')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./tokenizers/CharBPETokenizer/covid-vocab.json',\n",
              " './tokenizers/CharBPETokenizer/covid-merges.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfeI9AA-u2iE",
        "outputId": "cf827e48-eb84-40d0-db2b-97fb950ee519"
      },
      "source": [
        "sentencepiece_bpe_tokenizer = SentencePieceBPETokenizer()\n",
        "sentencepiece_bpe_tokenizer.train(files=[corpus_path], vocab_size=vocab_size)\n",
        "sentencepiece_bpe_tokenizer.save_model(directory='./tokenizers/SentencePieceBPETokenizer/', name='covid')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./tokenizers/SentencePieceBPETokenizer/covid-vocab.json',\n",
              " './tokenizers/SentencePieceBPETokenizer/covid-merges.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3glclcQvu2iJ",
        "outputId": "6661f13d-24b4-4e56-8572-02d87aa5f5ac"
      },
      "source": [
        "bert_wordpiece_tokenizer = BertWordPieceTokenizer()\n",
        "bert_wordpiece_tokenizer.train(\n",
        "    files=[corpus_path], vocab_size=vocab_size)\n",
        "bert_wordpiece_tokenizer.save_model(\n",
        "    directory='./tokenizers/BertWordPieceTokenizer/', name='covid')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./tokenizers/BertWordPieceTokenizer/covid-vocab.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVgp8OFSu2iO",
        "outputId": "e641a5ba-6c2a-48f3-8acc-fdf7eb8c2831"
      },
      "source": [
        "sent_ko = '신종 코로나바이러스 감염증(코로나19) 사태가 심각합니다'\n",
        "tokenizers = [bert_wordpiece_tokenizer,\n",
        "              sentencepiece_bpe_tokenizer,\n",
        "              char_bpe_tokenizer,\n",
        "              byte_level_bpe_tokenizer]\n",
        "\n",
        "for tokenizer in tokenizers:\n",
        "    encode_single = tokenizer.encode(sent_ko)\n",
        "    print(f'\\n{tokenizer.__class__.__name__}')\n",
        "    print(f'tokens = {encode_single.tokens}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "BertWordPieceTokenizer\n",
            "tokens = ['신종', '코로나바이러스', '감염증', '(', '코로나19', ')', '사태', '##가', '심', '##각', '##합니다']\n",
            "\n",
            "SentencePieceBPETokenizer\n",
            "tokens = ['▁신종', '▁코로나바이러스', '▁감염증(코로나19)', '▁사태', '가', '▁심', '각', '합', '니다']\n",
            "\n",
            "CharBPETokenizer\n",
            "tokens = ['신종</w>', '코로나바이러스</w>', '감염증</w>', '(</w>', '코로나19</w>', ')</w>', '사태', '가</w>', '심', '각', '합니다</w>']\n",
            "\n",
            "ByteLevelBPETokenizer\n",
            "tokens = ['ìĭłì¢ħ', 'Ġì½Ķë¡ľëĤĺë°ĶìĿ´ëŁ¬ìĬ¤', 'Ġê°ĲìĹ¼ì¦Ŀ', '(', 'ì½Ķë¡ľëĤĺ', '19', ')', 'ĠìĤ¬íĥľ', 'ê°Ģ', 'Ġìĭ¬', 'ê°ģ', 'íķ©ëĭĪëĭ¤']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vylxbWTFu2if"
      },
      "source": [
        "## 학습한 토크나이저를 transformers 에서 이용하자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYDwBa_9u2ig",
        "outputId": "bc7a30fb-7f2e-4207-8c97-adf7d388ae4b"
      },
      "source": [
        "from transformers import BertTokenizer, GPT2Tokenizer\n",
        "\n",
        "transformers_bert_tokenizer = BertTokenizer(\n",
        "    vocab_file = './tokenizers/BertWordPieceTokenizer/covid-vocab.txt'\n",
        ")\n",
        "print(f'tokenizers  : {bert_wordpiece_tokenizer.encode(sent_ko).tokens}')\n",
        "print(f'transformers: {transformers_bert_tokenizer.tokenize(sent_ko)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenizers  : ['신종', '코로나바이러스', '감염증', '(', '코로나19', ')', '사태', '##가', '심', '##각', '##합니다']\n",
            "transformers: ['신종', '코로나바이러스', '감염증', '(', '코로나19', ')', '사태', '##가', '심', '##각', '##합니다']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7DHY3Tmu2ik",
        "outputId": "6d1e919e-0676-4dd6-e1fa-085ef9282767"
      },
      "source": [
        "for token in bert_wordpiece_tokenizer.encode(sent_ko).tokens[:3]:\n",
        "    print(f'len({token}) = {len(token)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(신종) = 6\n",
            "len(코로나바이러스) = 14\n",
            "len(감염증) = 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPVhaSzwu2in",
        "outputId": "696a9d23-d1de-409a-edd1-2d544b657e35"
      },
      "source": [
        "from unicodedata import normalize\n",
        "\n",
        "print(normalize('NFKD', '가감'))  # 출력 시 글자를 재조합해서 보여줌\n",
        "print(len(normalize('NFKD', '가감')))\n",
        "print(normalize('NFKC', normalize('NFKD', '가감')))\n",
        "print(len(normalize('NFKC', normalize('NFKD', '가감'))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "가감\n",
            "5\n",
            "가감\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gw74mW5u2is",
        "outputId": "09a78147-8961-4306-e82d-ae00331b431d"
      },
      "source": [
        "from unicodedata import normalize\n",
        "\n",
        "def compose(tokens):\n",
        "    return [normalize('NFKC', token) for token in tokens]\n",
        "\n",
        "print(f'tokenizers  : {compose(bert_wordpiece_tokenizer.encode(sent_ko).tokens)}')\n",
        "print(f'transformers: {compose(transformers_bert_tokenizer.tokenize(sent_ko))}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenizers  : ['신종', '코로나바이러스', '감염증', '(', '코로나19', ')', '사태', '##가', '심', '##각', '##합니다']\n",
            "transformers: ['신종', '코로나바이러스', '감염증', '(', '코로나19', ')', '사태', '##가', '심', '##각', '##합니다']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "i_RdLPD0u2iy",
        "outputId": "78906c3d-42b3-48a9-e58d-62446223ae2e"
      },
      "source": [
        "transformers_gpt2_tokenizer = GPT2Tokenizer(\n",
        "    vocab_file = './tokenizers/ByteLevelBPETokenizer/covid-vocab.json',\n",
        "    merges_file = './tokenizers/ByteLevelBPETokenizer/covid-merges.txt'\n",
        ")\n",
        "print(f'tokenizers  : {byte_level_bpe_tokenizer.encode(sent_ko).tokens}')\n",
        "print(f'transformers: {transformers_gpt2_tokenizer.tokenize(sent_ko)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenizers  : ['ìĭłì¢ħ', 'Ġì½Ķë¡ľëĤĺë°ĶìĿ´ëŁ¬ìĬ¤', 'Ġê°ĲìĹ¼ì¦Ŀ', '(', 'ì½Ķë¡ľëĤĺ', '19', ')', 'ĠìĤ¬íĥľ', 'ê°Ģ', 'Ġìĭ¬', 'ê°ģ', 'íķ©ëĭĪëĭ¤']\n",
            "transformers: ['ìĭłì¢ħ', 'Ġì½Ķë¡ľëĤĺë°ĶìĿ´ëŁ¬ìĬ¤', 'Ġê°ĲìĹ¼ì¦Ŀ', '(', 'ì½Ķë¡ľëĤĺ', '19', ')', 'ĠìĤ¬íĥľ', 'ê°Ģ', 'Ġìĭ¬', 'ê°ģ', 'íķ©ëĭĪëĭ¤']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGn_ErAlu2i2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}