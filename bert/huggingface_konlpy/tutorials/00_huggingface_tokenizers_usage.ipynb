{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.8.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tokenizers\n",
    "tokenizers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (ByteLevelBPETokenizer,\n",
    "                        CharBPETokenizer,\n",
    "                        SentencePieceBPETokenizer,\n",
    "                        BertWordPieceTokenizer)\n",
    "\n",
    "small_corpus = 'very_small_corpus.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert WordPiece Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', '##B', '##C', '##D', '##E']\n",
      "[5, 11, 12, 13, 14]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer(\n",
    "    lowercase = False)\n",
    "bert_wordpiece_tokenizer.train(files=[small_corpus], vocab_size=10)\n",
    "encoding = bert_wordpiece_tokenizer.encode('ABCDE')\n",
    "print(encoding.tokens)\n",
    "print(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', 'a', 'b', 'c', 'd', 'e', 'f', '##f', '##b', '##c', '##d', '##e']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer()\n",
    "bert_wordpiece_tokenizer.train(\n",
    "    files = [small_corpus],\n",
    "    vocab_size = 10,\n",
    "    min_frequency = 1,\n",
    "    limit_alphabet = 1000,\n",
    "    initial_alphabet = [],\n",
    "    special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    show_progress = True,\n",
    "    wordpieces_prefix = \"##\",\n",
    ")\n",
    "vocab = bert_wordpiece_tokenizer.get_vocab()\n",
    "print(sorted(vocab, key=lambda x: vocab[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', '##b', '##c', '##d', '##e']\n",
      "[5, 12, 13, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "encoding = bert_wordpiece_tokenizer.encode('ABCDE')\n",
    "print(encoding.tokens)\n",
    "print(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', 'a', 'b', 'c', 'd', 'e', 'f', 'g', '##c', '##b', '##d', '##e', '##f', 'ab', 'abc', 'ac']\n"
     ]
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer()\n",
    "bert_wordpiece_tokenizer.train(\n",
    "    files = [small_corpus],\n",
    "    vocab_size = 20,\n",
    "    min_frequency = 1,\n",
    "    initial_alphabet = ['g'],\n",
    ")\n",
    "vocab = bert_wordpiece_tokenizer.get_vocab()\n",
    "print(sorted(vocab, key=lambda x: vocab[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', '##d', '##e']\n",
      "[18, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "encoding = bert_wordpiece_tokenizer.encode('ABCDE')\n",
    "print(encoding.tokens)\n",
    "print(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', '##d', '##e']\n",
      "['abc', '##d']\n"
     ]
    }
   ],
   "source": [
    "encodings = bert_wordpiece_tokenizer.encode_batch(['ABCDE', 'abcd'])\n",
    "print(encodings[0].tokens)\n",
    "print(encodings[1].tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./very_small_bertwordpiece-vocab.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer.save_model(\n",
    "    directory = './',\n",
    "    name = 'very_small_bertwordpiece'\n",
    ")\n",
    "# ['./very_small_bertwordpiece-vocab.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'abc', '##d', '##e', '[SEP]']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer(\n",
    "    vocab_file = './very_small_bertwordpiece-vocab.txt'\n",
    ")\n",
    "bert_wordpiece_tokenizer.encode('ABCDE').tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc', '##d', '##e']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer.encode('ABCDE', add_special_tokens=False).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'abc', '##d', '##e', '[SEP]', 'abc', '##d', '[SEP]']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer.encode(\n",
    "    sequence = 'abcde',\n",
    "    pair = 'abcd'\n",
    ").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', 'a', 'b', 'c', 'd', 'e', 'f', 'g', '##c', '##b', '##d', '##e', '##f', 'ab', 'abc', 'ac']\n"
     ]
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer.add_tokens(['lovit'])\n",
    "bert_wordpiece_tokenizer.save_model(\n",
    "    directory = './',\n",
    "    name = 'very_small_bertwordpiece_lovit')\n",
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer(\n",
    "    vocab_file = './very_small_bertwordpiece_lovit-vocab.txt'\n",
    ")\n",
    "vocab = bert_wordpiece_tokenizer.get_vocab()\n",
    "print(sorted(vocab, key=lambda x: vocab[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'abcde', '[UNK]', 'lovit', '[SEP]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer(\n",
    "    vocab_file = './very_small_bertwordpiece_lovit2-vocab.txt')\n",
    "bert_wordpiece_tokenizer.encode('ABCDE abg lovit').tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentencePiece BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', 'A', 'B', 'C', 'D', 'E', 'F', '▁', '▁A', '▁AB', '▁ABC', 'DE', '▁DE', '▁AC', '▁AF', '▁ABD', '▁ABCDE']\n"
     ]
    }
   ],
   "source": [
    "sentencepiece_tokenizer = SentencePieceBPETokenizer(\n",
    "    add_prefix_space = True)\n",
    "sentencepiece_tokenizer.train(\n",
    "    files = [small_corpus],\n",
    "    vocab_size = 20,\n",
    "    min_frequency = 1,\n",
    "    special_tokens = ['<unk>'],)\n",
    "vocab = sentencepiece_tokenizer.get_vocab()\n",
    "print(sorted(vocab, key=lambda x: vocab[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', 'lovit', 'A', 'B', 'C', 'D', 'E', 'F', '▁', '▁A', '▁AB', 'DE', '▁ABC', 'AB', 'CDE', '▁AC', '▁AF', '▁ABD', 'ABCDE']\n"
     ]
    }
   ],
   "source": [
    "sentencepiece_tokenizer = SentencePieceBPETokenizer(\n",
    "    add_prefix_space = False\n",
    ")\n",
    "sentencepiece_tokenizer.train(\n",
    "    files = [small_corpus],\n",
    "    vocab_size = 20,\n",
    "    min_frequency = 1,\n",
    "    special_tokens = ['<unk>', 'lovit'],\n",
    ")\n",
    "vocab = sentencepiece_tokenizer.get_vocab()\n",
    "print(sorted(vocab, key=lambda x: vocab[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./very_small_sentencepiece-vocab.json',\n",
       " './very_small_sentencepiece-merges.txt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentencepiece_tokenizer.save_model('./', 'very_small_sentencepiece')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁ABC', 'DE']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentencepiece_tokenizer = SentencePieceBPETokenizer(\n",
    "    vocab_file = './very_small_sentencepiece-vocab.json',\n",
    "    merges_file = './very_small_sentencepiece-merges.txt'\n",
    ")\n",
    "sentencepiece_tokenizer.encode('ABCDE').tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AB', 'C', 'DE</w>', 'ABC</w>']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charbpe_tokenizer = CharBPETokenizer(suffix='</w>')\n",
    "charbpe_tokenizer.train(\n",
    "    files = [small_corpus],\n",
    "    vocab_size = 15,\n",
    "    min_frequency = 1\n",
    ")\n",
    "charbpe_tokenizer.encode('ABCDE.ABC').tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AB', 'C', 'D', 'E', 'ABC</w>']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charbpe_tokenizer = CharBPETokenizer(\n",
    "    suffix='</w>',\n",
    "    split_on_whitespace_only = True\n",
    ")\n",
    "charbpe_tokenizer.train(\n",
    "    files = [small_corpus],\n",
    "    vocab_size = 15,\n",
    "    min_frequency = 1\n",
    ")\n",
    "charbpe_tokenizer.encode('ABCDE.ABC').tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "charbpe_tokenizer.train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AB', 'C', 'D', 'E', 'ABC</w>']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charbpe_tokenizer = CharBPETokenizer(\n",
    "    suffix='</w>',\n",
    "    split_on_whitespace_only = True\n",
    ")\n",
    "charbpe_tokenizer.train(\n",
    "    files = [small_corpus],\n",
    "    vocab_size = 15,\n",
    "    min_frequency = 1,\n",
    ")\n",
    "charbpe_tokenizer.encode('ABCDE.ABC').tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AB', 'C', 'D', 'E', 'F']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charbpe_tokenizer.encode('ABCDEFGH').tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-level BPE Tokenizer\n",
    "\n",
    "- OpenAI GPT2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '¡', '¢', '£', '¤', '¥', '¦', '§', '¨', '©', 'ª', '«', '¬', '®', '¯', '°', '±', '²', '³', '´', 'µ', '¶', '·', '¸', '¹', 'º', '»', '¼', '½', '¾', '¿', 'À', 'Á', 'Â', 'Ã', 'Ä', 'Å', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ï', 'Ð', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', '×', 'Ø', 'Ù', 'Ú', 'Û', 'Ü', 'Ý', 'Þ', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', '÷', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'þ', 'ÿ', 'Ā', 'ā', 'Ă', 'ă', 'Ą', 'ą', 'Ć', 'ć', 'Ĉ', 'ĉ', 'Ċ', 'ċ', 'Č', 'č', 'Ď', 'ď', 'Đ', 'đ', 'Ē', 'ē', 'Ĕ', 'ĕ', 'Ė', 'ė', 'Ę', 'ę', 'Ě', 'ě', 'Ĝ', 'ĝ', 'Ğ', 'ğ', 'Ġ', 'ġ', 'Ģ', 'ģ', 'Ĥ', 'ĥ', 'Ħ', 'ħ', 'Ĩ', 'ĩ', 'Ī', 'ī', 'Ĭ', 'ĭ', 'Į', 'į', 'İ', 'ı', 'Ĳ', 'ĳ', 'Ĵ', 'ĵ', 'Ķ', 'ķ', 'ĸ', 'Ĺ', 'ĺ', 'Ļ', 'ļ', 'Ľ', 'ľ', 'Ŀ', 'ŀ', 'Ł', 'ł', 'Ń', 'ĠA', 'ĠAB', 'ĠABC', 'DE', 'ĠDE', 'ĠAC', 'ĠAF', 'ĠABD', 'ĠABCDE']\n"
     ]
    }
   ],
   "source": [
    "bytebpe_tokenizer = ByteLevelBPETokenizer(add_prefix_space=True)\n",
    "bytebpe_tokenizer.train(files = [small_corpus],\n",
    "    vocab_size = 1000, min_frequency = 1)\n",
    "vocab = bytebpe_tokenizer.get_vocab()\n",
    "print(sorted(vocab, key=lambda x: vocab[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ĠABCDE', 'ĠABC']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytebpe_tokenizer.encode('ABCDE ABC').tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코로나19 관련 뉴스를 학습해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./tokenizers/ByteLevelBPETokenizer/covid-vocab.json',\n",
       " './tokenizers/ByteLevelBPETokenizer/covid-merges.txt']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import (ByteLevelBPETokenizer,\n",
    "                        CharBPETokenizer,\n",
    "                        SentencePieceBPETokenizer,\n",
    "                        BertWordPieceTokenizer)\n",
    "\n",
    "corpus_path = '../data/2020-07-29_covid_news_sents.txt'\n",
    "vocab_size = 3000\n",
    "\n",
    "byte_level_bpe_tokenizer = ByteLevelBPETokenizer()\n",
    "byte_level_bpe_tokenizer.train(files=[corpus_path], vocab_size=vocab_size)\n",
    "byte_level_bpe_tokenizer.save_model(directory='./tokenizers/ByteLevelBPETokenizer/', name='covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./tokenizers/CharBPETokenizer/covid-vocab.json',\n",
       " './tokenizers/CharBPETokenizer/covid-merges.txt']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_bpe_tokenizer = CharBPETokenizer()\n",
    "char_bpe_tokenizer.train(files=[corpus_path], vocab_size=vocab_size)\n",
    "char_bpe_tokenizer.save_model(directory='./tokenizers/CharBPETokenizer/', name='covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./tokenizers/SentencePieceBPETokenizer/covid-vocab.json',\n",
       " './tokenizers/SentencePieceBPETokenizer/covid-merges.txt']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentencepiece_bpe_tokenizer = SentencePieceBPETokenizer()\n",
    "sentencepiece_bpe_tokenizer.train(files=[corpus_path], vocab_size=vocab_size)\n",
    "sentencepiece_bpe_tokenizer.save_model(directory='./tokenizers/SentencePieceBPETokenizer/', name='covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./tokenizers/BertWordPieceTokenizer/covid-vocab.txt']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer()\n",
    "bert_wordpiece_tokenizer.train(\n",
    "    files=[corpus_path], vocab_size=vocab_size)\n",
    "bert_wordpiece_tokenizer.save_model(\n",
    "    directory='./tokenizers/BertWordPieceTokenizer/', name='covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BertWordPieceTokenizer\n",
      "tokens = ['신종', '코로나바이러스', '감염증', '(', '코로나19', ')', '사태', '##가', '심', '##각', '##합니다']\n",
      "\n",
      "SentencePieceBPETokenizer\n",
      "tokens = ['▁신종', '▁코로나바이러스', '▁감염증(코로나19)', '▁사태', '가', '▁심', '각', '합', '니다']\n",
      "\n",
      "CharBPETokenizer\n",
      "tokens = ['신종</w>', '코로나바이러스</w>', '감염증</w>', '(</w>', '코로나19</w>', ')</w>', '사태', '가</w>', '심', '각', '합니다</w>']\n",
      "\n",
      "ByteLevelBPETokenizer\n",
      "tokens = ['ìĭłì¢ħ', 'Ġì½Ķë¡ľëĤĺë°ĶìĿ´ëŁ¬ìĬ¤', 'Ġê°ĲìĹ¼ì¦Ŀ', '(', 'ì½Ķë¡ľëĤĺ', '19', ')', 'ĠìĤ¬íĥľ', 'ê°Ģ', 'Ġìĭ¬', 'ê°ģ', 'íķ©ëĭĪëĭ¤']\n"
     ]
    }
   ],
   "source": [
    "sent_ko = '신종 코로나바이러스 감염증(코로나19) 사태가 심각합니다'\n",
    "tokenizers = [bert_wordpiece_tokenizer,\n",
    "              sentencepiece_bpe_tokenizer,\n",
    "              char_bpe_tokenizer,\n",
    "              byte_level_bpe_tokenizer]\n",
    "\n",
    "for tokenizer in tokenizers:\n",
    "    encode_single = tokenizer.encode(sent_ko)\n",
    "    print(f'\\n{tokenizer.__class__.__name__}')\n",
    "    print(f'tokens = {encode_single.tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습한 토크나이저를 transformers 에서 이용하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers  : ['신종', '코로나바이러스', '감염증', '(', '코로나19', ')', '사태', '##가', '심', '##각', '##합니다']\n",
      "transformers: ['신종', '코로나바이러스', '감염증', '(', '코로나19', ')', '사태', '##가', '심', '##각', '##합니다']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, GPT2Tokenizer\n",
    "\n",
    "transformers_bert_tokenizer = BertTokenizer(\n",
    "    vocab_file = './tokenizers/BertWordPieceTokenizer/covid-vocab.txt'\n",
    ")\n",
    "print(f'tokenizers  : {bert_wordpiece_tokenizer.encode(sent_ko).tokens}')\n",
    "print(f'transformers: {transformers_bert_tokenizer.tokenize(sent_ko)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(신종) = 6\n",
      "len(코로나바이러스) = 14\n",
      "len(감염증) = 9\n"
     ]
    }
   ],
   "source": [
    "for token in bert_wordpiece_tokenizer.encode(sent_ko).tokens[:3]:\n",
    "    print(f'len({token}) = {len(token)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가감\n",
      "5\n",
      "가감\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "print(normalize('NFKD', '가감'))  # 출력 시 글자를 재조합해서 보여줌\n",
    "print(len(normalize('NFKD', '가감')))\n",
    "print(normalize('NFKC', normalize('NFKD', '가감')))\n",
    "print(len(normalize('NFKC', normalize('NFKD', '가감'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers  : ['신종', '코로나바이러스', '감염증', '(', '코로나19', ')', '사태', '##가', '심', '##각', '##합니다']\n",
      "transformers: ['신종', '코로나바이러스', '감염증', '(', '코로나19', ')', '사태', '##가', '심', '##각', '##합니다']\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "def compose(tokens):\n",
    "    return [normalize('NFKC', token) for token in tokens]\n",
    "\n",
    "print(f'tokenizers  : {compose(bert_wordpiece_tokenizer.encode(sent_ko).tokens)}')\n",
    "print(f'transformers: {compose(transformers_bert_tokenizer.tokenize(sent_ko))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers  : ['ìĭłì¢ħ', 'Ġì½Ķë¡ľëĤĺë°ĶìĿ´ëŁ¬ìĬ¤', 'Ġê°ĲìĹ¼ì¦Ŀ', '(', 'ì½Ķë¡ľëĤĺ', '19', ')', 'ĠìĤ¬íĥľ', 'ê°Ģ', 'Ġìĭ¬', 'ê°ģ', 'íķ©ëĭĪëĭ¤']\n",
      "transformers: ['ìĭłì¢ħ', 'Ġì½Ķë¡ľëĤĺë°ĶìĿ´ëŁ¬ìĬ¤', 'Ġê°ĲìĹ¼ì¦Ŀ', '(', 'ì½Ķë¡ľëĤĺ', '19', ')', 'ĠìĤ¬íĥľ', 'ê°Ģ', 'Ġìĭ¬', 'ê°ģ', 'íķ©ëĭĪëĭ¤']\n"
     ]
    }
   ],
   "source": [
    "transformers_gpt2_tokenizer = GPT2Tokenizer(\n",
    "    vocab_file = './tokenizers/ByteLevelBPETokenizer/covid-vocab.json',\n",
    "    merges_file = './tokenizers/ByteLevelBPETokenizer/covid-merges.txt'\n",
    ")\n",
    "print(f'tokenizers  : {byte_level_bpe_tokenizer.encode(sent_ko).tokens}')\n",
    "print(f'transformers: {transformers_gpt2_tokenizer.tokenize(sent_ko)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
