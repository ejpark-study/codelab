{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KoNLPy as pre-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신종 코로나바이러스 감염증 ( 코로나 19 ) 사태 가 심각 하 ㅂ니다\n"
     ]
    }
   ],
   "source": [
    "from huggingface_konlpy.tokenizers_konlpy import KoNLPyPreTokenizer\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "sent_ko = '신종 코로나바이러스 감염증(코로나19) 사태가 심각합니다'\n",
    "komoran_pretok = KoNLPyPreTokenizer(Komoran())\n",
    "print(komoran_pretok(sent_ko))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_konlpy.tokenizers_konlpy import KoNLPyPretokBertWordPieceTokenizer\n",
    "from huggingface_konlpy.transformers_konlpy import KoNLPyPretokBertTokenizer\n",
    "\n",
    "\n",
    "komoran_bertwordpiece_tokenizer = KoNLPyPretokBertWordPieceTokenizer(\n",
    "    konlpy_pretok = komoran_pretok)\n",
    "\n",
    "komoran_bertwordpiece_tokenizer.train(\n",
    "    files = ['../data/2020-07-29_covid_news_sents.txt'],\n",
    "    vocab_size = 3000)\n",
    "komoran_bertwordpiece_tokenizer.save_model(\n",
    "    directory='./tokenizers/KomoranBertWordPieceTokenizer/',\n",
    "    name='covid')\n",
    "\n",
    "komoran_pretok_berttokenizer = KoNLPyPretokBertTokenizer(\n",
    "    konlpy_pretok = komoran_pretok,\n",
    "    vocab_file = './tokenizers/KomoranBertWordPieceTokenizer/covid-vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 신종 코로나바이러스 감염증 ( 코로나 19 ) 사태 가 심 ##각 하 [UNK] [SEP]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_konlpy import compose\n",
    "\n",
    "indices = komoran_pretok_berttokenizer.encode(sent_ko)\n",
    "tokens = [komoran_pretok_berttokenizer.ids_to_tokens[ids] for ids in indices]\n",
    "print(' '.join(compose(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KoNLPy WordPiece Tokenizer\n",
    "\n",
    "### with tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신종 코로나 ##바이러스 감염증 ##( ##코로나 ##19 ##) 사태 ##가 심각 ##합니다\n"
     ]
    }
   ],
   "source": [
    "from huggingface_konlpy.tokenizers_konlpy import KoNLPyWordPieceTokenizer\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab_wordpiece_notag = KoNLPyWordPieceTokenizer(Mecab(), use_tag=False)\n",
    "print(' '.join(mecab_wordpiece_notag.tokenize(sent_ko)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신종/NNG 코로나/NNP ##바이러스/NNG 감염증/NNG ##(/SSO ##코로나/NNP ##19/SN ##)/SSC 사태/NNG ##가/JKS 심각/XR ##합니다/XSA+EC\n"
     ]
    }
   ],
   "source": [
    "mecab_wordpiece_usetag = KoNLPyWordPieceTokenizer(Mecab(), use_tag=True)\n",
    "print(' '.join(mecab_wordpiece_usetag.tokenize(sent_ko)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialize alphabet 1/1: 100%|██████████| 70964/70964 [00:00<00:00, 80584.70it/s]\n",
      "Train vocab 1/1: 100%|██████████| 70964/70964 [00:14<00:00, 4790.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[/mnt/lovit/git/transformers_konlpy_trainer/tutorials/tokenizers/BertStyleMecab/notag-vocab.txt]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_konlpy.tokenizers_konlpy import KoNLPyBertWordPieceTrainer\n",
    "\n",
    "mecab_wordpiece_notag_trainer = KoNLPyBertWordPieceTrainer(\n",
    "    Mecab(), use_tag=False)\n",
    "mecab_wordpiece_notag_trainer.train(\n",
    "    files = ['../data/2020-07-29_covid_news_sents.txt'])\n",
    "mecab_wordpiece_notag_trainer.save_model('./tokenizers/BertStyleMecab/', 'notag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신종 코로나 ##바이러스 감염증 ##( ##코로나 ##19 ##) 사태 ##가 심각 ##합니다\n"
     ]
    }
   ],
   "source": [
    "from huggingface_konlpy.transformers_konlpy import KoNLPyBertTokenizer\n",
    "\n",
    "konlpy_bert_notag = KoNLPyBertTokenizer(\n",
    "    konlpy_wordpiece = KoNLPyWordPieceTokenizer(Mecab(), use_tag=False),\n",
    "    vocab_file = './tokenizers/BertStyleMecab/notag-vocab.txt'\n",
    ")\n",
    "print(' '.join(konlpy_bert_notag.tokenize(sent_ko)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialize alphabet 1/1: 100%|██████████| 70964/70964 [00:00<00:00, 81053.35it/s]\n",
      "Train vocab 1/1: 100%|██████████| 70964/70964 [00:14<00:00, 4826.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[/mnt/lovit/git/transformers_konlpy_trainer/tutorials/tokenizers/BertStyleMecab/usetag-vocab.txt]\n",
      "[CLS] 신종/NNG 코로나/NNP ##바이러스/NNG 감염증/NNG ##(/SSO ##코로나/NNP ##19/SN ##)/SSC 사태/NNG ##가/JKS 심각/XR 합 니 다 [SEP]\n"
     ]
    }
   ],
   "source": [
    "mecab_wordpiece_usetag_trainer = KoNLPyBertWordPieceTrainer(Mecab(), use_tag=True)\n",
    "mecab_wordpiece_usetag_trainer.train(\n",
    "    files = ['../data/2020-07-29_covid_news_sents.txt'])\n",
    "mecab_wordpiece_usetag_trainer.save_model('./tokenizers/BertStyleMecab/', 'usetag')\n",
    "\n",
    "konlpy_bert_usetag = KoNLPyBertTokenizer(\n",
    "    konlpy_wordpiece = KoNLPyWordPieceTokenizer(Mecab(), use_tag=True),\n",
    "    vocab_file = './tokenizers/BertStyleMecab/usetag-vocab.txt')\n",
    "\n",
    "indices = konlpy_bert_usetag.encode(sent_ko)\n",
    "tokens = [konlpy_bert_usetag.ids_to_tokens[ids] for ids in indices]\n",
    "print(' '.join(compose(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
